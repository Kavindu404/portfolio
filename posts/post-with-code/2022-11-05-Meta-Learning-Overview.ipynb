{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8b8370a",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Meta Learning\"\n",
    "author: \"Kavindu Piyumal\"\n",
    "date: \"2022-11-12\"\n",
    "categories: [code, analysis]\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a2fec4",
   "metadata": {},
   "source": [
    "# Meta Learning - An Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df820ebe",
   "metadata": {},
   "source": [
    "Recent development of meta learning has driven the industry to a complete different trajectory compared to the ordinary and traditional learning algorithms. In ordinary way, we have fixed learning algorithms to perform specific tasks. But in meta learning, we try to improve the learning algorithm so that it is able to perform a range of tasks from the same distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6cce5a",
   "metadata": {},
   "source": [
    "Let's take a conventional supervised learning as an example. To start with the task, we will need a dataset _D_ that is consisting of tuples of (x, y) where x refers to data points and y refers to labels. Then, we train a predictive model to predict labels; $\\hat{y}$ = $f_{\\theta}$(x). Note that the model is parameterized by $\\theta$. We tarin the predictive model by solving;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8da0d43",
   "metadata": {},
   "source": [
    "$\\theta^{*}$ = arg $min_{\\theta}$ _L_(_D_;$\\theta$, _w_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55557f7",
   "metadata": {},
   "source": [
    "In here, _L_ is a loss function that measures the loss between the targets given in dataset _D_ and predictions of $f_{\\theta}$. And we try to minimize the loss by adapting the parameters $\\theta$ to get the optimal set of parameters $\\theta^{*}$. The _w_ here refers to initial assumptions we made of optimization and learning function. This is what meta learning try to tackle. Instead of having a fixed w, what if we implement an algorithm that can learn to choose the best _w_ when a new task is given? In that way, we will be able to generalize our learning algorithms for a range of tasks.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b493e9f",
   "metadata": {},
   "source": [
    "Lke with the usual machine learning approach, in meta learning too we have a training step and a testing step. Let's look into both steps one at a time. In the training step, we assume a a set _M_ of relatable tasks sampled from _p_(_T_). Then, we have a collection of tasks same as we have a collection of data points and labels in the conventional way. However, each element in the collection is consisting of a validation dataset and a training dataset.i.e., \n",
    "_D_$_{source}$ = (_D_$_{source}^{val}$, _D_$_{source}^{train}$)$^{(i)}$ for i=0, 1, ... , M."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041f694e",
   "metadata": {},
   "source": [
    "Then, we can define the training step as below;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b563888",
   "metadata": {},
   "source": [
    "$w^{*}$ = arg $min_{w}$ $\\sum \\limits_{i=1}^{M}$ _L_ $(D_{source}^{(i)};w)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1330df6e",
   "metadata": {},
   "source": [
    "So, in the training step, we try to get the optimal w, i.e., $w^{*}$ by giving it  the target tasks sampled from the distribution _p_(_T_)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d322935e",
   "metadata": {},
   "source": [
    "You see, in the conventional way, we always had the w, such as optimizers and learning functions fixed but in meta-learning, we train them in the training step. Once,the training is done, we move onto the testing stage. In there, we use the $w^{*}$ as the input along with a set of unseen tasks to train the base model on those unseen tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ca2b46",
   "metadata": {},
   "source": [
    "$\\theta^{*}$ = arg $min_{\\theta}$ _L_($D_{target}^{train (i)}$;$\\theta$, $w^{*}$)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6e8f3b",
   "metadata": {},
   "source": [
    "So, this is same as the conventional way but with the additional advantage of not having a fixed _w_ but an adaptive $w^{*}$ that allow us to use it in a variety of tasks within a similar kind of distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf29dba",
   "metadata": {},
   "source": [
    "So, hope you were able to get some idea about meta learning and its background. If you want to look into this further, I suggest [this review paper](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9428530) which was the reference for this article."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
