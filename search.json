[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Florida State University | Tallahassee, FL | MSc in Computational Science | Aug 2023 - Present\nSri Lanka Institute of Information Technology | Sri Lanka | BSc in Electrical and Electronic Engineering | Jan 2019 - Jan 2023"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About",
    "section": "",
    "text": "Florida State University | Tallahassee, FL | MSc in Computational Science | Aug 2023 - Present\nSri Lanka Institute of Information Technology | Sri Lanka | BSc in Electrical and Electronic Engineering | Jan 2019 - Jan 2023"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "About",
    "section": "Experience",
    "text": "Experience\nCenter for Ocean-Atmospheric Prediction Studies | Graduate Research Assistant | Nov 2023 - Present\nAtlas Labs | Data Scientist | Jan 2023 - Jul 2023\nZone24x7 Pvt. Ltd. | Machine Learning Intern | Sep 2021 - Dec 2022"
  },
  {
    "objectID": "posts/post-with-code/2022-10-20-What-is-Stable-Diffusion.html",
    "href": "posts/post-with-code/2022-10-20-What-is-Stable-Diffusion.html",
    "title": "Stable Diffusion Explained",
    "section": "",
    "text": "What is Stable Diffusion?\nStable diffusion has made a revolutionary entry to world changing lots of state-of-the-art methods that was existing for such a long time. But what is stable diffusion? What are the main components of it? What is the architecture of it? There are so many questions! So let’stry to get a high-level understanding of what stable diffusionis.\nFirst, let’s take our usual mnist digit classifier for an example. As shown in the figure below, if we give it a handwritten image as the input, it will predict the value of that digit.\n\n\n\nimage.png\n\n\nBut, what if we input the same image but with some noise now as shown below? Most of the time the model will be able to predict it correctly right?\n\n\n\nimage%20%281%29.png\n\n\nSo, what does this mean? Basically, this means that our model is capable of differentiating the noise from the actual information.Isn’t this amazing? But, is there any way we can use this to generate images? This is exactly what diffusion models do. They start with pure noise and then they goes through a denoising process to get to the new generated image. Let’s see what modifications we have to do in order to get there.\nFirstly, in the above mentioned example, we have a multi-class classification case. So we will be using a backbone like ResNet as the model and change the head of the model accordingly. But, when it comes to the image generation, we want to get an image as the output. So what kind of a model should we use? Think of a similar kind of an application, where we input an image and get an image as the output. Did you guess it? Yes, we use a U-Net as our model.\nBut, how do we use U-Net architecture to generate an image? So the catch is, we do not generate an image as the output. Think that we input an image with certain information with some amount of noise. We train our U-Net to predict the noise of the image. Then, we get the output, which is the noise, and substract it from the input to get the denoised image. The process is as shown below.\n\n\n\nimage%20%282%29.png\n\n\nOkay,there are so many things to explain I know. But lets+’s get to there one step at a time. First thing to note is that, the U-Net is not able to predict the noise from just one step (as of October 2022, things change so fast :P). It will use something like a feedback loop here. In other words, it will predict the noise from the initial input and then, it will substract and the noise from the input and use the new image as the input in the next step. Likewise, it will continue this process for multiple number of steps.\nBut the thing is, the U-Net architectures are so computationally expensive. It will work fine for mnist as they are 128x128 images, but what if we use an input of size 512x512. It will take hours right? So to reduce the computational complexity, we use a variational encoder. As you know, in a VAE, we have an encoder and an decoder. We first use the encoder to get input latents to the U-Net and continue the denoising process explained above to denoice the noisy latents. Then, we get the output of the U-Net and input that to the decoder to get the output image. Pretty cool right?!\nSo you may ask now, how we can generate new images if we have to input the image we want to generate with some added noise to get back the image itself? The catch is, we input pure noise. That’s right. So our model starts with pure noise and then follow up with a denoising process to generate new images. But, if that’s the case, the output can be any random image right? Because, there is no assuarance that this will output ameaningful image by starting with pure noise without any guidance. Aha! we need some guidance! So, how we give guidance to our model to let them know about what kind of an output we are looking for? The answer is, we use a prompt. So a prompt is a description of the output we seeking for. But we cannot input a string to the model. It only recognizes features. So we use a text encoder to get encodings of the prompt. This is where CLIP comes in (I will cover the concept of CLIP in a future blog, for now, just think of it as a model that outputs text encodings).\nSo now, we have everything we needs. We first initialize a random image of the output size we looking for which has to be a power of 2. Then, we use the encoder of the VAE to get the noisy latents. We parallely get the encodings of the prompt too. Then, we input this into our U-Net model and iteratively predict the noise for a predefined number of steps. Then, we get the output of the U-Net and input that into the decoder to get the spatial domain generated image.\nHope, this blog gave you a high-level understanding of the idea of stable diffusion. In the coming blogs, I will dive into the mathematical aspect of diffusion models. To follow them, you will need to have some knowledge on random processes, Gaussian (Normal) distribution, and some calculus. So be ready!"
  },
  {
    "objectID": "posts/post-with-code/2022-10-03-Multiclass -Classification -from -Scratch -(Part 1).html",
    "href": "posts/post-with-code/2022-10-03-Multiclass -Classification -from -Scratch -(Part 1).html",
    "title": "Multi Class Classification From Scratch (Part 1)",
    "section": "",
    "text": "Multi Class Classification From Scratch (Part 1)\nIn this mini blog series, I am implementing multiclass classifier for MNIST digits from scratch. In this part, I will be classifying the digits using pixel similarity. I will try to improve the performance in each part. First, let’s import FastAI\n\nfrom fastai.vision.all import *\n\nMNIST dataset can be downloaded and extracted using untar_data() method. With FastAI, we can easily list the elements in the extracted derectory.\n\npath = untar_data(URLs.MNIST)\nPath.BASE_PATH = path\npath.ls()\n\n(#2) [Path('testing'),Path('training')]\n\n\nLet’s first get training data into different objects. The ls() method returns an object of class L in FastAI.It has all the functionalities in python list() and some more.\n\nzeros = (path/'training'/'0').ls().sorted()\nones = (path/'training'/'1').ls().sorted()\ntwos = (path/'training'/'2').ls().sorted()\nthrees = (path/'training'/'3').ls().sorted()\nfours = (path/'training'/'4').ls().sorted()\nfives = (path/'training'/'5').ls().sorted()\nsixes = (path/'training'/'6').ls().sorted()\nsevens = (path/'training'/'7').ls().sorted()\neights = (path/'training'/'8').ls().sorted()\nnines = (path/'training'/'9').ls().sorted()\n\n\nzeros,ones,twos,threes,fours,fives,sixes,sevens,eights,nines\n\n((#5923) [Path('training/0/1.png'),Path('training/0/1000.png'),Path('training/0/10005.png'),Path('training/0/10010.png'),Path('training/0/10022.png'),Path('training/0/10025.png'),Path('training/0/10026.png'),Path('training/0/10045.png'),Path('training/0/10069.png'),Path('training/0/10071.png')...],\n (#6742) [Path('training/1/10006.png'),Path('training/1/10007.png'),Path('training/1/1002.png'),Path('training/1/10020.png'),Path('training/1/10027.png'),Path('training/1/1003.png'),Path('training/1/10040.png'),Path('training/1/10048.png'),Path('training/1/10058.png'),Path('training/1/10067.png')...],\n (#5958) [Path('training/2/10009.png'),Path('training/2/10016.png'),Path('training/2/10024.png'),Path('training/2/10029.png'),Path('training/2/10072.png'),Path('training/2/10073.png'),Path('training/2/10075.png'),Path('training/2/10078.png'),Path('training/2/10081.png'),Path('training/2/10082.png')...],\n (#6131) [Path('training/3/10.png'),Path('training/3/10000.png'),Path('training/3/10011.png'),Path('training/3/10031.png'),Path('training/3/10034.png'),Path('training/3/10042.png'),Path('training/3/10052.png'),Path('training/3/1007.png'),Path('training/3/10074.png'),Path('training/3/10091.png')...],\n (#5842) [Path('training/4/10013.png'),Path('training/4/10018.png'),Path('training/4/10033.png'),Path('training/4/1004.png'),Path('training/4/1006.png'),Path('training/4/10060.png'),Path('training/4/1008.png'),Path('training/4/10103.png'),Path('training/4/10104.png'),Path('training/4/10114.png')...],\n (#5421) [Path('training/5/0.png'),Path('training/5/100.png'),Path('training/5/10008.png'),Path('training/5/10015.png'),Path('training/5/10030.png'),Path('training/5/10035.png'),Path('training/5/10049.png'),Path('training/5/10051.png'),Path('training/5/10056.png'),Path('training/5/10062.png')...],\n (#5918) [Path('training/6/10017.png'),Path('training/6/10032.png'),Path('training/6/10036.png'),Path('training/6/10037.png'),Path('training/6/10044.png'),Path('training/6/10053.png'),Path('training/6/10076.png'),Path('training/6/10089.png'),Path('training/6/10101.png'),Path('training/6/10108.png')...],\n (#6265) [Path('training/7/10002.png'),Path('training/7/1001.png'),Path('training/7/10014.png'),Path('training/7/10019.png'),Path('training/7/10039.png'),Path('training/7/10046.png'),Path('training/7/10050.png'),Path('training/7/10063.png'),Path('training/7/10077.png'),Path('training/7/10086.png')...],\n (#5851) [Path('training/8/10001.png'),Path('training/8/10012.png'),Path('training/8/10021.png'),Path('training/8/10041.png'),Path('training/8/10054.png'),Path('training/8/10057.png'),Path('training/8/10061.png'),Path('training/8/10064.png'),Path('training/8/10066.png'),Path('training/8/10079.png')...],\n (#5949) [Path('training/9/10003.png'),Path('training/9/10004.png'),Path('training/9/10023.png'),Path('training/9/10028.png'),Path('training/9/10038.png'),Path('training/9/10043.png'),Path('training/9/10047.png'),Path('training/9/1005.png'),Path('training/9/10055.png'),Path('training/9/10059.png')...])\n\n\nNow that we have all the data seperated into objects, let’s stack them up.\n\nstacked_zeros = torch.stack([tensor(Image.open(o)) for o in zeros]).float()/255\nstacked_ones = torch.stack([tensor(Image.open(o)) for o in ones]).float()/255\nstacked_twos = torch.stack([tensor(Image.open(o)) for o in twos]).float()/255\nstacked_threes = torch.stack([tensor(Image.open(o)) for o in threes]).float()/255\nstacked_fours = torch.stack([tensor(Image.open(o)) for o in fours]).float()/255\nstacked_fives = torch.stack([tensor(Image.open(o)) for o in fives]).float()/255\nstacked_sixes = torch.stack([tensor(Image.open(o)) for o in sixes]).float()/255\nstacked_sevens = torch.stack([tensor(Image.open(o)) for o in sevens]).float()/255\nstacked_eights = torch.stack([tensor(Image.open(o)) for o in eights]).float()/255\nstacked_nines = torch.stack([tensor(Image.open(o)) for o in nines]).float()/255\n\n\nstacked_zeros.shape, stacked_ones.shape, stacked_twos.shape, stacked_threes.shape, stacked_fours.shape, stacked_fives.shape, stacked_sixes.shape, stacked_sevens.shape, stacked_eights.shape, stacked_nines.shape, \n\n(torch.Size([5923, 28, 28]),\n torch.Size([6742, 28, 28]),\n torch.Size([5958, 28, 28]),\n torch.Size([6131, 28, 28]),\n torch.Size([5842, 28, 28]),\n torch.Size([5421, 28, 28]),\n torch.Size([5918, 28, 28]),\n torch.Size([6265, 28, 28]),\n torch.Size([5851, 28, 28]),\n torch.Size([5949, 28, 28]))\n\n\nIn our first attempt, we will use pixel similarity. So, first, let’s calculate the mean for each digit.\n\nmean0 = stacked_zeros.mean(0)\nmean1 = stacked_ones.mean(0)\nmean2 = stacked_twos.mean(0)\nmean3 = stacked_threes.mean(0)\nmean4 = stacked_fours.mean(0)\nmean5 = stacked_fives.mean(0)\nmean6 = stacked_sixes.mean(0)\nmean7 = stacked_sevens.mean(0)\nmean8 = stacked_eights.mean(0)\nmean9 = stacked_nines.mean(0)\n\nThe mean for each digit represents the ‘ideal’ digit that is expected. Let’s take a look at the ‘ideal’ 2.\n\ndf1 = pd.DataFrame(mean2[0:29,0:23])\ndf1.style.set_properties(**{'font-size':'4.5pt'}).background_gradient('Greys')\n\n\n\n\n\n\n \n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n\n\n\n\n0\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000007\n0.000142\n0.000142\n0.000006\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n1\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000011\n0.000184\n0.000234\n0.000400\n0.000471\n0.000259\n0.000369\n0.000718\n0.000733\n0.001320\n0.000560\n0.000047\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n2\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000030\n0.000301\n0.001010\n0.002387\n0.004827\n0.007155\n0.010033\n0.012604\n0.014091\n0.015811\n0.015780\n0.012331\n0.008118\n0.004130\n0.001792\n0.000594\n0.000146\n0.000008\n\n\n3\n0.000000\n0.000000\n0.000007\n0.000010\n0.000056\n0.001060\n0.004104\n0.010762\n0.024223\n0.045967\n0.070977\n0.100678\n0.127955\n0.151479\n0.162052\n0.160046\n0.140632\n0.108345\n0.072146\n0.043444\n0.021461\n0.008759\n0.002672\n\n\n4\n0.000000\n0.000000\n0.000042\n0.000090\n0.000819\n0.006367\n0.017791\n0.042089\n0.083421\n0.142241\n0.211767\n0.292313\n0.370494\n0.431761\n0.461901\n0.456149\n0.417914\n0.336929\n0.243804\n0.160297\n0.089220\n0.040629\n0.012857\n\n\n5\n0.000000\n0.000000\n0.000077\n0.000273\n0.003049\n0.016349\n0.045503\n0.096712\n0.171761\n0.263224\n0.362981\n0.463160\n0.547239\n0.610263\n0.644565\n0.647456\n0.608250\n0.530123\n0.407579\n0.281021\n0.173409\n0.088314\n0.033626\n\n\n6\n0.000000\n0.000000\n0.000053\n0.000680\n0.006591\n0.028900\n0.076292\n0.151805\n0.242472\n0.341647\n0.436100\n0.516924\n0.570205\n0.603516\n0.622732\n0.634839\n0.627733\n0.584579\n0.494698\n0.369531\n0.244626\n0.136664\n0.056396\n\n\n7\n0.000000\n0.000000\n0.000254\n0.000965\n0.010361\n0.040237\n0.099327\n0.176339\n0.259723\n0.339625\n0.407273\n0.454401\n0.472272\n0.475716\n0.483725\n0.502393\n0.532844\n0.546947\n0.510726\n0.415592\n0.291848\n0.174315\n0.079336\n\n\n8\n0.000000\n0.000045\n0.000288\n0.001272\n0.011794\n0.044498\n0.102871\n0.166226\n0.228230\n0.278922\n0.319372\n0.333384\n0.328697\n0.318813\n0.326290\n0.363595\n0.431744\n0.499352\n0.508834\n0.437793\n0.316499\n0.190417\n0.088839\n\n\n9\n0.000000\n0.000000\n0.000267\n0.001539\n0.011091\n0.039959\n0.085990\n0.129770\n0.165140\n0.192895\n0.213386\n0.212083\n0.195751\n0.186895\n0.201606\n0.266368\n0.371373\n0.475721\n0.510332\n0.450774\n0.323187\n0.191269\n0.087439\n\n\n10\n0.000000\n0.000000\n0.000130\n0.001378\n0.007421\n0.029077\n0.060358\n0.081593\n0.101843\n0.116352\n0.123059\n0.116682\n0.104919\n0.104716\n0.142872\n0.234735\n0.366604\n0.481890\n0.516487\n0.450063\n0.313075\n0.176987\n0.075751\n\n\n11\n0.000000\n0.000000\n0.000000\n0.000766\n0.005266\n0.018284\n0.033207\n0.043408\n0.053382\n0.062493\n0.064077\n0.060981\n0.061179\n0.081220\n0.144231\n0.261926\n0.398805\n0.500654\n0.514638\n0.435455\n0.288501\n0.150212\n0.058968\n\n\n12\n0.000000\n0.000000\n0.000000\n0.000292\n0.003241\n0.010065\n0.016726\n0.023751\n0.033803\n0.041343\n0.046381\n0.055371\n0.073395\n0.119073\n0.205897\n0.329536\n0.446791\n0.518040\n0.503421\n0.401478\n0.248775\n0.118531\n0.044476\n\n\n13\n0.000000\n0.000000\n0.000032\n0.000380\n0.002123\n0.007130\n0.016208\n0.028661\n0.045382\n0.063764\n0.087072\n0.119171\n0.161834\n0.229378\n0.320824\n0.418851\n0.502115\n0.529520\n0.473732\n0.349567\n0.202185\n0.091573\n0.035927\n\n\n14\n0.000000\n0.000000\n0.000000\n0.000631\n0.003544\n0.013647\n0.035809\n0.069251\n0.108837\n0.153193\n0.204482\n0.261094\n0.323508\n0.397957\n0.470263\n0.534277\n0.559740\n0.528731\n0.437916\n0.300230\n0.168766\n0.080268\n0.038659\n\n\n15\n0.000000\n0.000000\n0.000069\n0.002358\n0.011354\n0.039603\n0.090194\n0.155878\n0.224798\n0.295254\n0.366840\n0.438308\n0.511417\n0.578410\n0.619545\n0.628696\n0.593969\n0.520419\n0.403039\n0.271179\n0.162587\n0.093511\n0.061005\n\n\n16\n0.000000\n0.000000\n0.000350\n0.006034\n0.030797\n0.089589\n0.177901\n0.269840\n0.358496\n0.437472\n0.502642\n0.569108\n0.634937\n0.679840\n0.690350\n0.666253\n0.606259\n0.510305\n0.395925\n0.281850\n0.196662\n0.140591\n0.106600\n\n\n17\n0.000000\n0.000000\n0.000648\n0.014614\n0.062288\n0.152536\n0.268917\n0.373017\n0.453545\n0.510708\n0.556134\n0.610725\n0.656431\n0.686115\n0.683940\n0.651334\n0.590322\n0.509432\n0.420484\n0.334785\n0.265909\n0.211504\n0.168157\n\n\n18\n0.000000\n0.000000\n0.000675\n0.025045\n0.096128\n0.211982\n0.339039\n0.442429\n0.504724\n0.550516\n0.594093\n0.627278\n0.656336\n0.670207\n0.655604\n0.622225\n0.578423\n0.524000\n0.467761\n0.405183\n0.347683\n0.294237\n0.237217\n\n\n19\n0.000000\n0.000000\n0.001110\n0.032870\n0.117689\n0.242600\n0.378972\n0.491348\n0.566807\n0.618971\n0.651180\n0.668537\n0.669368\n0.650098\n0.613270\n0.575419\n0.541508\n0.513792\n0.488711\n0.452865\n0.406129\n0.347430\n0.274100\n\n\n20\n0.000000\n0.000009\n0.001803\n0.034276\n0.119279\n0.234893\n0.377356\n0.504160\n0.607640\n0.673011\n0.690975\n0.682460\n0.644020\n0.586525\n0.527870\n0.482407\n0.461993\n0.455073\n0.457337\n0.437818\n0.394623\n0.330521\n0.248283\n\n\n21\n0.000000\n0.000122\n0.001963\n0.026058\n0.091249\n0.188354\n0.313729\n0.440901\n0.550462\n0.618395\n0.625133\n0.591058\n0.529502\n0.456810\n0.395368\n0.356391\n0.344007\n0.349045\n0.354380\n0.343302\n0.304571\n0.245597\n0.176619\n\n\n22\n0.000000\n0.000000\n0.001066\n0.012793\n0.047181\n0.108409\n0.194192\n0.289751\n0.372962\n0.419609\n0.424284\n0.390612\n0.336123\n0.277596\n0.234563\n0.209485\n0.204407\n0.207852\n0.212406\n0.202193\n0.177242\n0.140470\n0.098799\n\n\n23\n0.000000\n0.000000\n0.000076\n0.002722\n0.012580\n0.032933\n0.063455\n0.101813\n0.135328\n0.154938\n0.158624\n0.146665\n0.126948\n0.106317\n0.091593\n0.081452\n0.079375\n0.077296\n0.076020\n0.071620\n0.063231\n0.051806\n0.035349\n\n\n24\n0.000000\n0.000000\n0.000000\n0.000052\n0.000743\n0.002455\n0.005081\n0.008940\n0.011869\n0.014211\n0.015518\n0.016728\n0.017879\n0.017852\n0.017593\n0.016117\n0.015332\n0.013918\n0.012382\n0.010117\n0.008858\n0.007724\n0.006063\n\n\n25\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000181\n0.000274\n0.000274\n0.000280\n0.000440\n0.000629\n0.000767\n0.001215\n0.001755\n0.001911\n0.001793\n0.001603\n0.001268\n0.000905\n0.000796\n0.000546\n0.000224\n\n\n26\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n27\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n\n\n\n\nim = stacked_ones[1]\nshow_image(im)\n\n\n\n\n\n\n\n\nNow, let’s collect validation dataset and stack them up.\n\nvalid_zeros = (path/'testing'/'0').ls().sorted()\nvalid_ones = (path/'testing'/'1').ls().sorted()\nvalid_twos = (path/'testing'/'2').ls().sorted()\nvalid_threes = (path/'testing'/'3').ls().sorted()\nvalid_fours = (path/'testing'/'4').ls().sorted()\nvalid_fives = (path/'testing'/'5').ls().sorted()\nvalid_sixes = (path/'testing'/'6').ls().sorted()\nvalid_sevens = (path/'testing'/'7').ls().sorted()\nvalid_eights = (path/'testing'/'8').ls().sorted()\nvalid_nines = (path/'testing'/'9').ls().sorted()\n\n\nvalid_stacked_zeros = torch.stack([tensor(Image.open(o)) for o in valid_zeros]).float()/255\nvalid_stacked_ones = torch.stack([tensor(Image.open(o)) for o in valid_ones]).float()/255\nvalid_stacked_twos = torch.stack([tensor(Image.open(o)) for o in valid_twos]).float()/255\nvalid_stacked_threes = torch.stack([tensor(Image.open(o)) for o in valid_threes]).float()/255\nvalid_stacked_fours = torch.stack([tensor(Image.open(o)) for o in valid_fours]).float()/255\nvalid_stacked_fives = torch.stack([tensor(Image.open(o)) for o in valid_fives]).float()/255\nvalid_stacked_sixes = torch.stack([tensor(Image.open(o)) for o in valid_sixes]).float()/255\nvalid_stacked_sevens = torch.stack([tensor(Image.open(o)) for o in valid_sevens]).float()/255\nvalid_stacked_eights = torch.stack([tensor(Image.open(o)) for o in valid_eights]).float()/255\nvalid_stacked_nines = torch.stack([tensor(Image.open(o)) for o in valid_nines]).float()/255\n\nIn order to get the pixel similarity, we have to get the distance from the ‘ideal’ digit for each digit. First, we have to check the distance for each ‘ideal’ digit and then choose the closest one. In distance() method, we simply get the distance between two inputs. In min_distance() method, we find the closest ‘ideal’ digit for a given input. In is_correct() method, we can simply determine whether our prediction using pixel similarity is correct or not.\n\ndef distance(x,y): return (x-y).abs().mean((-1,-2))\n\n\nmean_vec = [mean0, mean1, mean2, mean3, mean4, mean5, mean6, mean7, mean8, mean9]\ndef min_distance(x): \n    distances = [distance(x, o) for o in mean_vec]\n    return distances.index(min(distances))\n\n\ndef is_correct(num, x): return num == min_distance(x)\n\nLet’s check with some inputs.\n\nis_correct(4, valid_stacked_ones[140])\n\nFalse\n\n\nNow that we have guranteed it is working fine, let’s calculate the accuracy of the model. In here, we will simply get the correct prediction per each class and then get the mean of it.\n\nacc_zeros = tensor([is_correct(0,o) for o in valid_stacked_zeros]).float().mean()\nacc_ones = tensor([is_correct(1,o) for o in valid_stacked_ones]).float().mean()\nacc_twos = tensor([is_correct(2,o) for o in valid_stacked_twos]).float().mean()\nacc_threes = tensor([is_correct(3,o) for o in valid_stacked_threes]).float().mean()\nacc_fours = tensor([is_correct(4,o) for o in valid_stacked_fours]).float().mean()\nacc_fives = tensor([is_correct(5,o) for o in valid_stacked_fives]).float().mean()\nacc_sixes = tensor([is_correct(6,o) for o in valid_stacked_sixes]).float().mean()\nacc_sevens = tensor([is_correct(7,o) for o in valid_stacked_sevens]).float().mean()\nacc_eights = tensor([is_correct(8,o) for o in valid_stacked_eights]).float().mean()\nacc_nines = tensor([is_correct(9,o) for o in valid_stacked_nines]).float().mean()\n\nacc= tensor([acc_zeros, acc_ones, acc_twos, acc_threes, acc_fours, acc_fives, acc_sixes, acc_sevens, acc_eights, acc_nines]).mean()\nacc\n\ntensor(0.6610)\n\n\nSo, we have an accuracy of 66.1%. Given that we only considered pixel similarity, it is a good result. In next part, let’s try to improve from here."
  },
  {
    "objectID": "posts/post-with-code/2022-11-05-Meta-Learning-Overview.html",
    "href": "posts/post-with-code/2022-11-05-Meta-Learning-Overview.html",
    "title": "Meta Learning",
    "section": "",
    "text": "Meta Learning - An Overview\nRecent development of meta learning has driven the industry to a complete different trajectory compared to the ordinary and traditional learning algorithms. In ordinary way, we have fixed learning algorithms to perform specific tasks. But in meta learning, we try to improve the learning algorithm so that it is able to perform a range of tasks from the same distribution.\nLet’s take a conventional supervised learning as an example. To start with the task, we will need a dataset D that is consisting of tuples of (x, y) where x refers to data points and y refers to labels. Then, we train a predictive model to predict labels; \\(\\hat{y}\\) = \\(f_{\\theta}\\)(x). Note that the model is parameterized by \\(\\theta\\). We tarin the predictive model by solving;\n\\(\\theta^{*}\\) = arg \\(min_{\\theta}\\) L(D;\\(\\theta\\), w)\nIn here, L is a loss function that measures the loss between the targets given in dataset D and predictions of \\(f_{\\theta}\\). And we try to minimize the loss by adapting the parameters \\(\\theta\\) to get the optimal set of parameters \\(\\theta^{*}\\). The w here refers to initial assumptions we made of optimization and learning function. This is what meta learning try to tackle. Instead of having a fixed w, what if we implement an algorithm that can learn to choose the best w when a new task is given? In that way, we will be able to generalize our learning algorithms for a range of tasks.\nLke with the usual machine learning approach, in meta learning too we have a training step and a testing step. Let’s look into both steps one at a time. In the training step, we assume a a set M of relatable tasks sampled from p(T). Then, we have a collection of tasks same as we have a collection of data points and labels in the conventional way. However, each element in the collection is consisting of a validation dataset and a training dataset.i.e., D\\(_{source}\\) = (D\\(_{source}^{val}\\), D\\(_{source}^{train}\\))\\(^{(i)}\\) for i=0, 1, … , M.\nThen, we can define the training step as below;\n\\(w^{*}\\) = arg \\(min_{w}\\) \\(\\sum \\limits_{i=1}^{M}\\) L \\((D_{source}^{(i)};w)\\)\nSo, in the training step, we try to get the optimal w, i.e., \\(w^{*}\\) by giving it the target tasks sampled from the distribution p(T).\nYou see, in the conventional way, we always had the w, such as optimizers and learning functions fixed but in meta-learning, we train them in the training step. Once,the training is done, we move onto the testing stage. In there, we use the \\(w^{*}\\) as the input along with a set of unseen tasks to train the base model on those unseen tasks.\n\\(\\theta^{*}\\) = arg \\(min_{\\theta}\\) L(\\(D_{target}^{train (i)}\\);\\(\\theta\\), \\(w^{*}\\))\nSo, this is same as the conventional way but with the additional advantage of not having a fixed w but an adaptive \\(w^{*}\\) that allow us to use it in a variety of tasks within a similar kind of distribution.\nSo, hope you were able to get some idea about meta learning and its background. If you want to look into this further, I suggest this review paper which was the reference for this article."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog",
    "section": "",
    "text": "Meta Learning\n\n\n\n\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nNov 12, 2022\n\n\nKavindu Piyumal\n\n\n\n\n\n\n\n\n\n\n\n\nBroadcasting\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nOct 28, 2022\n\n\nKavindu Piyumal\n\n\n\n\n\n\n\n\n\n\n\n\nStable Diffusion Explained\n\n\n\n\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nOct 20, 2022\n\n\nKavindu Piyumal\n\n\n\n\n\n\n\n\n\n\n\n\nMulti Class Classification From Scratch (Part 2)\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nJul 25, 2022\n\n\nKavindu Piyumal\n\n\n\n\n\n\n\n\n\n\n\n\nMulti Class Classification From Scratch (Part 1)\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nJul 15, 2022\n\n\nKavindu Piyumal\n\n\n\n\n\n\n\n\n\n\n\n\nRandom Variables Explained\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nJul 12, 2022\n\n\nKavindu Piyumal\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/post-with-code/2022-10-28-Broadcasting-in-Python.html",
    "href": "posts/post-with-code/2022-10-28-Broadcasting-in-Python.html",
    "title": "Broadcasting",
    "section": "",
    "text": "Broadcasting\nIn this blog, I am going to breakdown the idea of broadcasting in python. Broadcasting is one of the main concepts that you should know about to get the best out of deep learning frameworks. Let’s see what is it.\nTo start with, say you have two variables, x and y as below;\n\nx = torch.tensor(10)\ny = torch.tensor([100,200,300])\n\nNow, let’s say, you want to add x for each element in y. So we can simply do as follows;\n\nz = x + y;z\n\ntensor([110, 210, 310])\n\n\nBut, how does this actually work? x is just a scalar and we just add that into a tensor. This is done with the help of broadcasting. Let’s see how it work under the hood. Say we have two tensors, a and b as below;\n\na = torch.tensor([1., 2, 3])\nb = torch.tensor([[10.,20,30],\n                  [40,50,60]])\na,b\n\n(tensor([1., 2., 3.]),\n tensor([[10., 20., 30.],\n         [40., 50., 60.]]))\n\n\n\na.shape, b.shape\n\n(torch.Size([3]), torch.Size([2, 3]))\n\n\nLet’s see what happen if we add them up …\n\nc = a + b;c\n\ntensor([[11., 22., 33.],\n        [41., 52., 63.]])\n\n\n\nc.shape\n\ntorch.Size([2, 3])\n\n\nSo, what actually happened is, pytorch done broadcasting on tensor a to get it to the same size as b and then done element wise multiplication. In other words, first, pytorch has expanded a to match the dimensions of b and then done the element wise multiplication as shown below;\n\na.expand_as(b)\n\ntensor([[1., 2., 3.],\n        [1., 2., 3.]])\n\n\nSee? we just replicate the tensor a to match it to the dimensions of tensor b. But, one may ask, how come this so useful if it gonna fill up the memory with the copiesof the same data right? The answer is it does not copy the same data to the memory. It only contains the original data that we given in the initialization.\n\na.storage()\n\n 1.0\n 2.0\n 3.0\n[torch.storage._TypedStorage(dtype=torch.float32, device=cpu) of size 3]\n\n\nLike that, it only contains the initial data we given. Then how does it perform the broadcasting? Well, the pytorch use a neat trick with strides to copy elements in the memory to get the matching dimentions.\n\nw = a.expand_as(b)\nw, w.shape\n\n(tensor([[1., 2., 3.],\n         [1., 2., 3.]]),\n torch.Size([2, 3]))\n\n\n\nw.stride()\n\n(0, 1)\n\n\nWhat does this mean? So, when we initialized tensor a at the beginning, the values 1, 2, and 3 are put into adjacent memory cells. The stride shows the way we should fill up the positions in the target dimensions. The first element refers to axis 0 and the second element refers to axis 1. So, 1 in (0,1) means, skip one memory location at a time to get to the next column whereas 0 in (0,1) means that we do not skip memory locations in dimension 0, i.e., raw wise.\nSo, how can we get a higher dimensional array from a lower dimensional one? Well, there are two ways to do that. The first is to use unsqueeze(dim) and the second is to index our initial tensor with [None].\n\na, a.shape\n\n(tensor([1., 2., 3.]), torch.Size([3]))\n\n\n\na.unsqueeze(0), a[None,:]\n\n(tensor([[1., 2., 3.]]), tensor([[1., 2., 3.]]))\n\n\n\na.unsqueeze(0).shape, a[None,:].shape\n\n(torch.Size([1, 3]), torch.Size([1, 3]))\n\n\nWe can always skip trailing ‘:’s and you will see that in many cases. Furthermore, we can use’…’ to imply all the preceding dimensions\n\na[None].shape, a\n\n(torch.Size([1, 3]), tensor([1., 2., 3.]))\n\n\n\na[...,None].shape, a\n\n(torch.Size([3, 1]), tensor([1., 2., 3.]))\n\n\nAs you see, we can expand the dimensions easily. The argument we pass into unsqueeze() is the position of the nex axis we want to add.\nHowever, there are certain rules associated with tensor operations; - Two tensors are compatible to perform tensor operations if their dimensions, starting from right to left; - equal or - one of them is one in which case we use broadcasting to get that one to the same dimension\nAnd that’s a wrap! Hope you were able to understand the concept of broadcasting a little more intuitively."
  },
  {
    "objectID": "posts/post-with-code/2022-10-03-Multiclass -Classification -from -Scratch -(Part 2).html",
    "href": "posts/post-with-code/2022-10-03-Multiclass -Classification -from -Scratch -(Part 2).html",
    "title": "Multi Class Classification From Scratch (Part 2)",
    "section": "",
    "text": "Multi Class Classification From Scratch (Part 2)\nWelcome to part 2 of the mini blog series on multi class classification from scratch. In first part, we used pixel similarity to predict the class of the digit. In this one, we will be building a simple neural network from scratch. Same as the last one, we start with importing fastai vision library and stacking up the data w.r.t classes like we did in the previous part.\n\nfrom fastai.vision.all import *\n\n\npath = untar_data(URLs.MNIST)\nPath.BASE_PATH = path\npath.ls()\n\n\n\n\n\n\n    \n      \n      100.03% [15687680/15683414 00:01&lt;00:00]\n    \n    \n\n\n(#2) [Path('training'),Path('testing')]\n\n\n\nzeros = (path/'training'/'0').ls().sorted()\nones = (path/'training'/'1').ls().sorted()\ntwos = (path/'training'/'2').ls().sorted()\nthrees = (path/'training'/'3').ls().sorted()\nfours = (path/'training'/'4').ls().sorted()\nfives = (path/'training'/'5').ls().sorted()\nsixes = (path/'training'/'6').ls().sorted()\nsevens = (path/'training'/'7').ls().sorted()\neights = (path/'training'/'8').ls().sorted()\nnines = (path/'training'/'9').ls().sorted()\n\n\nstacked_zeros = torch.stack([tensor(Image.open(o)) for o in zeros]).float()/255\nstacked_ones = torch.stack([tensor(Image.open(o)) for o in ones]).float()/255\nstacked_twos = torch.stack([tensor(Image.open(o)) for o in twos]).float()/255\nstacked_threes = torch.stack([tensor(Image.open(o)) for o in threes]).float()/255\nstacked_fours = torch.stack([tensor(Image.open(o)) for o in fours]).float()/255\nstacked_fives = torch.stack([tensor(Image.open(o)) for o in fives]).float()/255\nstacked_sixes = torch.stack([tensor(Image.open(o)) for o in sixes]).float()/255\nstacked_sevens = torch.stack([tensor(Image.open(o)) for o in sevens]).float()/255\nstacked_eights = torch.stack([tensor(Image.open(o)) for o in eights]).float()/255\nstacked_nines = torch.stack([tensor(Image.open(o)) for o in nines]).float()/255\n\n\ntrain_x = torch.cat([stacked_zeros, stacked_ones, stacked_twos, stacked_threes, stacked_fours, stacked_fives, stacked_sixes, stacked_sevens, stacked_eights, stacked_nines]).view(-1, 28*28)\ntrain_y = tensor([0]*len(zeros)+[1]*len(ones)+[2]*len(twos)+[3]*len(threes)+[4]*len(fours)+[5]*len(fives)+[6]*len(sixes)+[7]*len(sevens)+[8]*len(eights)+[9]*len(nines)).unsqueeze(1)\ntrain_x.shape, train_y.shape\n\n(torch.Size([60000, 784]), torch.Size([60000, 1]))\n\n\nHere, we create a dataset by coupling the image with its label. We do the same for the validation set.\n\ntrain_dset = list(zip(train_x, train_y))\nx, y = train_dset[100]\nx.shape, y.shape\n\n(torch.Size([784]), torch.Size([1]))\n\n\n\nvalid_zeros = (path/'testing'/'0').ls().sorted()\nvalid_ones = (path/'testing'/'1').ls().sorted()\nvalid_twos = (path/'testing'/'2').ls().sorted()\nvalid_threes = (path/'testing'/'3').ls().sorted()\nvalid_fours = (path/'testing'/'4').ls().sorted()\nvalid_fives = (path/'testing'/'5').ls().sorted()\nvalid_sixes = (path/'testing'/'6').ls().sorted()\nvalid_sevens = (path/'testing'/'7').ls().sorted()\nvalid_eights = (path/'testing'/'8').ls().sorted()\nvalid_nines = (path/'testing'/'9').ls().sorted()\n\n\nvalid_stacked_zeros = torch.stack([tensor(Image.open(o)) for o in valid_zeros]).float()/255\nvalid_stacked_ones = torch.stack([tensor(Image.open(o)) for o in valid_ones]).float()/255\nvalid_stacked_twos = torch.stack([tensor(Image.open(o)) for o in valid_twos]).float()/255\nvalid_stacked_threes = torch.stack([tensor(Image.open(o)) for o in valid_threes]).float()/255\nvalid_stacked_fours = torch.stack([tensor(Image.open(o)) for o in valid_fours]).float()/255\nvalid_stacked_fives = torch.stack([tensor(Image.open(o)) for o in valid_fives]).float()/255\nvalid_stacked_sixes = torch.stack([tensor(Image.open(o)) for o in valid_sixes]).float()/255\nvalid_stacked_sevens = torch.stack([tensor(Image.open(o)) for o in valid_sevens]).float()/255\nvalid_stacked_eights = torch.stack([tensor(Image.open(o)) for o in valid_eights]).float()/255\nvalid_stacked_nines = torch.stack([tensor(Image.open(o)) for o in valid_nines]).float()/255\n\n\nvalid_x = torch.cat([valid_stacked_zeros, valid_stacked_ones, valid_stacked_twos, valid_stacked_threes, valid_stacked_fours, valid_stacked_fives, valid_stacked_sixes, valid_stacked_sevens, valid_stacked_eights, valid_stacked_nines]).view(-1, 28*28)\nvalid_y = tensor([0]*len(valid_zeros)+[1]*len(valid_ones)+[2]*len(valid_twos)+[3]*len(valid_threes)+[4]*len(valid_fours)+[5]*len(valid_fives)+[6]*len(valid_sixes)+[7]*len(valid_sevens)+[8]*len(valid_eights)+[9]*len(valid_nines)).unsqueeze(1)\nvalid_x.shape, valid_y.shape\n\n(torch.Size([10000, 784]), torch.Size([10000, 1]))\n\n\n\nvalid_dset = list(zip(valid_x, valid_y))\n\nIn the FastAI book, Jeremy explain a 7-step process that we gonna follow. The steps are as below;\n\nInitialize the weights.\nFor each image, use these weights to predict the class.\nBased on these predictions, calculate how good the model is (its loss).\nCalculate the gradient, which measures for each weight, how changing that weight would change the loss\nStep (that is, change) all the weights based on that calculation.\nGo back to the step 2, and repeat the process.\nIterate until you decide to stop the training process (for instance, because the model is good enough or you don’t want to wait any longer).\n\nSo first, we write a function to init the weights.\n\ndef init_param(size, std=1.0): return (torch.randn(size)*std).requires_grad_()\n\n\nweights = init_param((28*28, 10))\nbias = init_param(10)\nweights.shape\n\ntorch.Size([784, 10])\n\n\nIn order to predict the class, we need a small model. So in the following function, we take the data as input and outputs the predictions.\n\ndef linear1(xb): return xb@weights+bias\n\nThen, in the next step, we need a way to calculate the loss for the predictions. There are liple loss functions available depending on the application. In this case, we need a loss function that can compute the loss in a muti-categorical problem. Softmax is the most common loss function used in such cases. It requires all the predictions to sum to 1 and it tends to push the most likely activation to a much larger value compared to others.\n\ndef softmax(x): return torch.exp(x)/torch.exp(x).sum(dim=1, keepdim=True)\n\nSo now we define crossentropy loss to first apply softmax to the predictions and then to calculate the loss comparing them to the targets\n\ndef crossentropy_loss(inputs, targets):\n    activations = softmax(inputs)\n    return -activations[range(inputs.shape[0]), targets].log().mean()\n\nOkay now we create dataloader objects for both training and validation sets. Note that each data point is having 784 features (28 * 28) and a corresponding label.\n\ndl = DataLoader(train_dset, batch_size=256)\nxb,yb = first(dl)\nxb.shape, yb.shape\n\n(torch.Size([256, 784]), torch.Size([256, 1]))\n\n\n\nvalid_dl = DataLoader(valid_dset, batch_size=256)\n\nLet’s grab a subset of the training set and get their predictions\n\nbatch = train_x[:4]\nbatch.shape\n\ntorch.Size([4, 784])\n\n\nNote that for each input, there are 10 outputs. Once we input them to the softmax loss function, it outputs those 10 values in a way that they add up to 1 with the most probable labels with higher values.\n\npreds = linear1(batch)\npreds.shape\n\ntorch.Size([4, 10])\n\n\n\nloss = crossentropy_loss(preds, train_y[:4])\nloss\n\ntensor(16.7473, grad_fn=&lt;NegBackward0&gt;)\n\n\n\nloss.backward()\nweights.grad.shape,weights.grad.mean(),bias.grad\n\n(torch.Size([784, 10]),\n tensor(2.4328e-10),\n tensor([-9.9999e-01,  7.6655e-01,  1.7359e-07,  5.4389e-04,  5.4135e-02,\n          3.6396e-07,  1.7450e-01,  4.2626e-03,  3.9071e-06,  2.6428e-07]))\n\n\nNow we put all the steps together to calculate the gradients\n\ndef calc_grad(xb, yb, model):\n    preds = model(xb)\n    loss = crossentropy_loss(preds, yb)\n    loss.backward()\n\nNext we define the following function to train our model for a single epoch. It will take each batch from the data loader and will calculate the gradients. Then it will update the weights accordingly\n\ndef train_epoch(model, lr, param):\n    for xb, yb in dl:\n        calc_grad(xb, yb, model)\n        for p in params:\n            p.data -= p.grad*lr\n            p.grad.zero_()\n\nFinally, we can package them allto train for multiple epochs\n\ndef train_model(model,lr, param, epochs):\n    val_acc = []\n    for i in range (epochs):\n        train_epoch(model, lr, param)\n        val_acc.append(validate_epoch(model))\n    return val_acc\n\nWe also define functions to calculate train and validation accuracies.\n\ndef batch_accuracy(preds,yb):\n    correct=preds.max(axis=1)[1]==yb\n    return correct.float().mean()\n\n\ndef validate_epoch(model):\n    accs = [batch_accuracy(model(xb), yb) for xb, yb in valid_dl]\n    return round(torch.stack(accs).mean().item(), 4)\n\n\nweights = init_param((28*28, 10))\nbias = init_param(10)\nparams = weights,bias\nlr = 1e-1\nval_acc=(train_model(linear1, lr, params, 100))\nplt.plot(range(100), val_acc), val_acc[-1]\n\n\n\n\n\n\n\n\nSo our simple neural network is capable of classifying digits with a validation accuracy of 0.621"
  },
  {
    "objectID": "posts/post-with-code/2022-10-12-Random-Variables-and-How-to-Implement-them-in-Python.html",
    "href": "posts/post-with-code/2022-10-12-Random-Variables-and-How-to-Implement-them-in-Python.html",
    "title": "Random Variables Explained",
    "section": "",
    "text": "Random Variables\nIn this blog, I am trying to give an introduction to random variables and follow up with an implementation of random numbers in python.\nBut first, what is a random variable?\nThere are multip definitions to random variables depending on the context. The main purpose of using random variables are so that we can define certain probability functions to make it convinient and easy to compute the probabilitiesf certain events. Consider a random experiment with a sample space $$\nA random variable X(\\(\\xi\\)) is a single valued real function that assigns a real number to each sample point \\(\\xi\\) of \\(\\Gamma\\). In other words, a random variable is not a variable but a function. Okay, let’s take a simple example.\nImagine a toss of a coin and we only do it once. Now, we can define a random variable X as below;\n                    X(H) = 1, X(T) = 0\nWhat just happened there? Let’s breakit down. Now, our sample space \\(\\Gamma\\) consist of two random events; H and T, where H is for heads and T is for tails. So, our random variable X is just a function that assigns real values for those events. Let’s define two other random variables Y, and Z. We can define them as below;\n                    Y(H) = 0, Y(T) = 1\n                    Z(H) = 0, Z(T) = 0\nSo finally, if X is arandom variable and x is a fixed number, we can define the event (X=x) as:\n(X = x) = {\\(\\xi\\): X(\\(\\xi\\)) = x}\nwhich is read as a set of all the events in a particular sample space such that X(\\(\\xi\\)) is equal to x. One other thing to notice is that each random variable has its own distribution and they assign real values according to their respective distriutions.\nSo, now with the background knowledge of the random variables, let’s see how to generate random numbers in python. Random generators in python are like random variables. Each time we call them, they give us a real value.\nHowever, unfortunately, there is no way of creating random numbers with software, rather we generate pseudo random numbers. Pseudo random number generators are used to generate lots and lots of random numbers rather than just one random number. The following implementation of the randomnumber generator is based on Wichmann Hill algorithm.\nA pseudo number generator is a mathematical function that we can call and each time we call it, it will generate a number that looks random.\n\nrnd_state = None\n\ndef seed(a):\n    global rnd_state\n    a, x = divmod(a, 30268)\n    a, y = divmod(a, 30306)\n    a, z = divmod(a, 30322)\n    rnd_state = int(x)+1, int(y)+1, int(z)+1\n\nThe key thing to understand here is the global variable rnd_state. Let’s get some outputs for rnd_state;\n\nseed(76216435476523170)\nrnd_state\n\n(543, 10388, 5335)\n\n\nrnd-state now contains three numbers.The seed we passed here is just a random seed that I smashed on the keyboard. Now, we pass this rnd_state to the following function that generates pseudo random numbers.\n\ndef rand():\n    global rnd_state\n    x, y, z = rnd_state\n    x = (171 * x) % 30269\n    y = (172 * y) % 30307\n    z = (170 * z) % 30323\n    rnd_state = x, y, z\n    return (x/30269 + y/30307 + z/30323) % 1.0\n\nNow we can call the rand() function multiple times and it will generate different pseudo random numbers each time;\n\nrand(), rand(), rand()\n\n(0.931798407488099, 0.3824530849180521, 0.9459261819993035)\n\n\nSo now we have a pseudo random number generator that we can call to generate random numbers!!!!!\nLet’s check whether we get different numbers if we call say 50 times.\n\nfrom matplotlib import pyplot as plt\n\n\nplt.plot([rand() for _ in range(50)])\n\n\n\n\n\n\n\n\nIt does!! One other thing to notice is that all the random numebrs are normalized to 1. If you take a closer look at the return statement in the rand() function,you will see that we take the modulo of 1 which makes sense for the generated results. So, if our function works, if we plot the histogram of results for a lots and lots of generated numbers, it should contain all the values between 0 and 1. Let’s see that as well;\n\nplt.hist([rand() for _ in range (1000000)])\n\n(array([ 99852.,  99946., 100376., 100087.,  99555.,  99533., 100245.,\n        100235.,  99813., 100358.]),\n array([1.24317537e-07, 1.00000093e-01, 2.00000062e-01, 3.00000031e-01,\n        4.00000000e-01, 4.99999969e-01, 5.99999938e-01, 6.99999907e-01,\n        7.99999876e-01, 8.99999845e-01, 9.99999814e-01]),\n &lt;a list of 10 Patch objects&gt;)\n\n\n\n\n\n\n\n\n\nHoorayyy!! It works too!!!"
  }
]